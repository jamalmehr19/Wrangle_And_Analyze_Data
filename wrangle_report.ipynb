{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I went through the process of wrangling of data from Twitter account called `WeRateDogs`. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. In this report I briefly describe my wrangling efforts for this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started my project with the main data wrangling process consisted of:\n",
    "- Gathering data\n",
    "- Assessing data\n",
    "- Cleaning data\n",
    "The remaining of the project was:\n",
    "- Storing data\n",
    "- Analyzing and visualizing data\n",
    "- And finally derive insights from the analyzed clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling started with gathering data. I needed to gather needed data from three different sources using three different methods.\n",
    " - The first was to manually download the WeRateDogs Twitter archive file which was hosted on Udacity's website. I downloaded the file and put it in my project folder.\n",
    " - The second was to programatically downloading the tweet image predictions file having the link to the file url and using Requests library.\n",
    " - The third was to use tweet IDs in Twitter archive file, query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file. Each tweet's JSON data should be written to its own line. Then read this .txt file line by line into a pandas DataFrame with tweet ID, retweet count, favorite count, and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After gathering all needed data I assessed data for 17 data quality and tidiness issues. The list of all issues is included in the `wrangle_act` file. The assessment was performed both visually and programatically. The visual assessment was done in Jupyter Notebook printing the whole dataframe and also random sampling data. Programatic assessment was performed using different tools such as .info, .value_counts(), .query, .describe(), .duplicated(), .loc, nunique(), and isin()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each quality and wrangling issue was cleaned following three steps of define, code, and test. I started the cleaning with making a copy of each dataframe to come back and make a copy again in case I made a mistake in cleaning process. The issue was described in define step and the essential code was generated to resolve the issue in code section. The final section of test was to assure if the code worked fine and the issue was resolved completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three cleaned dataframes were merged into a master dataframe and compiled as a single cleaned dataset. I wrote the master dataset into a csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing and Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several visualiations including bar plots, histograms, pie charts, donut charts, scatter plots, and heat maps were created to analyze data and derive insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Insights from Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dedicated this part to derive insights and conclude my project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
